{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. JRA Horse Racing Data Exploration\n",
    "\n",
    "This notebook explores the Kaggle JRA Horse Racing Dataset to understand:\n",
    "- Data structure and available features\n",
    "- Data quality and missing values\n",
    "- Key statistics for horse racing prediction\n",
    "- Feature engineering opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "RAW_DATA_DIR = Path('../data/raw')\n",
    "\n",
    "# List all CSV files\n",
    "csv_files = list(RAW_DATA_DIR.glob('*.csv'))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {f.name} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files into a dictionary\n",
    "dfs = {}\n",
    "for f in csv_files:\n",
    "    name = f.stem\n",
    "    print(f\"Loading {name}...\")\n",
    "    dfs[name] = pd.read_csv(f)\n",
    "    print(f\"  Shape: {dfs[name].shape}\")\n",
    "    print(f\"  Columns: {list(dfs[name].columns)[:10]}...\" if len(dfs[name].columns) > 10 else f\"  Columns: {list(dfs[name].columns)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed info for each dataframe\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DataFrame: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumn types:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values for each dataframe\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Missing values: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(missing_df)\n",
    "    else:\n",
    "        print(\"No missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Race Data Analysis\n",
    "\n",
    "Assuming there's a main race results table. Adjust column names based on actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the main results dataframe (likely the largest or named 'results', 'race', etc.)\n",
    "# This will need adjustment based on actual file names\n",
    "main_df_name = None\n",
    "for name in dfs.keys():\n",
    "    if 'result' in name.lower() or 'race' in name.lower():\n",
    "        main_df_name = name\n",
    "        break\n",
    "\n",
    "if main_df_name is None and len(dfs) > 0:\n",
    "    # Use the largest dataframe as main\n",
    "    main_df_name = max(dfs.keys(), key=lambda x: len(dfs[x]))\n",
    "\n",
    "if main_df_name:\n",
    "    print(f\"Using '{main_df_name}' as main dataframe\")\n",
    "    df = dfs[main_df_name].copy()\n",
    "else:\n",
    "    print(\"No data loaded. Please download the dataset first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns\n",
    "if 'df' in dir():\n",
    "    print(\"All columns:\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"{i+1:3}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter Data for 2019-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for date column and filter to 2019-2021\n",
    "# Column names will vary - common patterns: 'date', 'race_date', 'year', etc.\n",
    "if 'df' in dir():\n",
    "    date_cols = [col for col in df.columns if any(x in col.lower() for x in ['date', 'year', '日付', '年'])]\n",
    "    print(f\"Potential date columns: {date_cols}\")\n",
    "    \n",
    "    # Try to parse and filter by year\n",
    "    for col in date_cols:\n",
    "        try:\n",
    "            if df[col].dtype == 'object':\n",
    "                df['_year'] = pd.to_datetime(df[col], errors='coerce').dt.year\n",
    "            else:\n",
    "                df['_year'] = df[col]\n",
    "            \n",
    "            year_counts = df['_year'].value_counts().sort_index()\n",
    "            print(f\"\\nYear distribution from '{col}':\")\n",
    "            print(year_counts)\n",
    "            \n",
    "            # Filter to 2019-2021\n",
    "            df_filtered = df[(df['_year'] >= 2019) & (df['_year'] <= 2021)].copy()\n",
    "            print(f\"\\nFiltered to 2019-2021: {len(df_filtered)} rows\")\n",
    "            df = df_filtered\n",
    "            break\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Statistics for Horse Racing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section will analyze race-specific statistics\n",
    "# Adjust column names based on actual data\n",
    "\n",
    "if 'df' in dir():\n",
    "    # Try to identify key columns\n",
    "    print(\"Sample of data:\")\n",
    "    display(df.head(10))\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nNumerical statistics:\")\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential feature columns based on naming patterns\n",
    "if 'df' in dir():\n",
    "    col_categories = {\n",
    "        'horse': ['horse', '馬', 'uma'],\n",
    "        'jockey': ['jockey', '騎手', 'kishu'],\n",
    "        'trainer': ['trainer', '調教', 'choukyou'],\n",
    "        'odds': ['odds', 'オッズ', '単勝', '複勝'],\n",
    "        'position': ['position', '着順', 'rank', 'order', '順位'],\n",
    "        'weight': ['weight', '体重', '斤量', 'kinryou'],\n",
    "        'distance': ['distance', '距離', 'kyori'],\n",
    "        'track': ['track', 'course', 'コース', '馬場', 'baba'],\n",
    "        'corner': ['corner', 'コーナー', '通過'],\n",
    "        'time': ['time', 'タイム', '秒', 'sec'],\n",
    "        'blood': ['sire', 'father', '父', 'dam', 'mother', '母'],\n",
    "    }\n",
    "    \n",
    "    print(\"Column categorization:\")\n",
    "    for category, patterns in col_categories.items():\n",
    "        matching_cols = [col for col in df.columns if any(p in col.lower() for p in patterns)]\n",
    "        if matching_cols:\n",
    "            print(f\"\\n{category.upper()}:\")\n",
    "            for col in matching_cols:\n",
    "                print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Finishing Position Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze finishing positions (target variable for prediction)\n",
    "if 'df' in dir():\n",
    "    # Try to find finishing position column\n",
    "    position_cols = [col for col in df.columns if any(x in col.lower() for x in ['着順', 'rank', 'position', 'order', 'finish'])]\n",
    "    \n",
    "    if position_cols:\n",
    "        pos_col = position_cols[0]\n",
    "        print(f\"Using '{pos_col}' as finishing position\")\n",
    "        \n",
    "        # Clean and analyze\n",
    "        df[pos_col] = pd.to_numeric(df[pos_col], errors='coerce')\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        df[pos_col].value_counts().sort_index().head(18).plot(kind='bar')\n",
    "        plt.title('Finishing Position Distribution')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        df[pos_col].value_counts().sort_index().head(18).cumsum().div(len(df)).mul(100).plot(kind='line', marker='o')\n",
    "        plt.title('Cumulative % by Position')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Cumulative %')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Odds Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze odds distribution and relationship with results\n",
    "if 'df' in dir():\n",
    "    odds_cols = [col for col in df.columns if any(x in col.lower() for x in ['odds', 'オッズ', '単勝', 'tansho'])]\n",
    "    \n",
    "    if odds_cols and position_cols:\n",
    "        odds_col = odds_cols[0]\n",
    "        pos_col = position_cols[0]\n",
    "        print(f\"Odds column: {odds_col}\")\n",
    "        \n",
    "        df[odds_col] = pd.to_numeric(df[odds_col], errors='coerce')\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Odds distribution for winners vs all\n",
    "        plt.subplot(1, 2, 1)\n",
    "        df[df[pos_col] == 1][odds_col].clip(upper=100).hist(bins=50, alpha=0.7, label='Winners')\n",
    "        df[odds_col].clip(upper=100).hist(bins=50, alpha=0.3, label='All')\n",
    "        plt.title('Odds Distribution')\n",
    "        plt.xlabel('Odds')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Win rate by odds range\n",
    "        plt.subplot(1, 2, 2)\n",
    "        df['odds_bin'] = pd.cut(df[odds_col], bins=[0, 2, 5, 10, 20, 50, 100, 1000])\n",
    "        win_rate = df.groupby('odds_bin')[pos_col].apply(lambda x: (x == 1).mean() * 100)\n",
    "        win_rate.plot(kind='bar')\n",
    "        plt.title('Win Rate by Odds Range')\n",
    "        plt.xlabel('Odds Range')\n",
    "        plt.ylabel('Win Rate (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Umatan (Exacta) Analysis\n",
    "\n",
    "For Umatan, we need to predict 1st and 2nd place in exact order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 1st-2nd combinations\n",
    "if 'df' in dir():\n",
    "    # Find race ID column\n",
    "    race_id_cols = [col for col in df.columns if any(x in col.lower() for x in ['race_id', 'レースid', 'race_no'])]\n",
    "    \n",
    "    if race_id_cols and position_cols:\n",
    "        race_col = race_id_cols[0] if race_id_cols else None\n",
    "        pos_col = position_cols[0]\n",
    "        \n",
    "        if race_col:\n",
    "            print(f\"Race column: {race_col}\")\n",
    "            \n",
    "            # Count horses per race\n",
    "            horses_per_race = df.groupby(race_col).size()\n",
    "            print(f\"\\nHorses per race:\")\n",
    "            print(f\"  Min: {horses_per_race.min()}\")\n",
    "            print(f\"  Max: {horses_per_race.max()}\")\n",
    "            print(f\"  Mean: {horses_per_race.mean():.1f}\")\n",
    "            print(f\"  Median: {horses_per_race.median():.1f}\")\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            horses_per_race.value_counts().sort_index().plot(kind='bar')\n",
    "            plt.title('Distribution of Horses per Race')\n",
    "            plt.xlabel('Number of Horses')\n",
    "            plt.ylabel('Number of Races')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Track and Distance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by track surface and distance\n",
    "if 'df' in dir():\n",
    "    distance_cols = [col for col in df.columns if any(x in col.lower() for x in ['distance', '距離', 'kyori'])]\n",
    "    track_cols = [col for col in df.columns if any(x in col.lower() for x in ['芝', 'turf', 'dirt', 'ダート', 'surface', '馬場'])]\n",
    "    \n",
    "    if distance_cols:\n",
    "        dist_col = distance_cols[0]\n",
    "        df[dist_col] = pd.to_numeric(df[dist_col], errors='coerce')\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        df[dist_col].value_counts().sort_index().plot(kind='bar')\n",
    "        plt.title('Race Distance Distribution')\n",
    "        plt.xlabel('Distance (m)')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        if track_cols:\n",
    "            track_col = track_cols[0]\n",
    "            plt.subplot(1, 2, 2)\n",
    "            df[track_col].value_counts().plot(kind='bar')\n",
    "            plt.title('Track Surface Distribution')\n",
    "            plt.xlabel('Surface')\n",
    "            plt.ylabel('Count')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Racecourse Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JRA Racecourses\n",
    "RACECOURSE_CODES = {\n",
    "    1: \"札幌\", 2: \"函館\", 3: \"福島\", 4: \"新潟\", 5: \"東京\",\n",
    "    6: \"中山\", 7: \"中京\", 8: \"京都\", 9: \"阪神\", 10: \"小倉\"\n",
    "}\n",
    "\n",
    "if 'df' in dir():\n",
    "    course_cols = [col for col in df.columns if any(x in col.lower() for x in ['場', 'course', 'racecourse', 'venue', 'place'])]\n",
    "    \n",
    "    if course_cols:\n",
    "        course_col = course_cols[0]\n",
    "        print(f\"Racecourse column: {course_col}\")\n",
    "        print(f\"\\nRaces by racecourse:\")\n",
    "        \n",
    "        course_counts = df[course_col].value_counts()\n",
    "        print(course_counts)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        course_counts.plot(kind='bar')\n",
    "        plt.title('Races by Racecourse')\n",
    "        plt.xlabel('Racecourse')\n",
    "        plt.ylabel('Number of Runners')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Engineering Ideas\n",
    "\n",
    "Based on the data exploration, here are potential features to engineer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of data structure for feature engineering\n",
    "if 'df' in dir():\n",
    "    print(\"=\"*60)\n",
    "    print(\"DATA SUMMARY FOR FEATURE ENGINEERING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTotal rows: {len(df):,}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"\\nColumn types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDED FEATURES TO ENGINEER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    features = [\n",
    "        \"1. Running Style (脚質) - from corner passing positions\",\n",
    "        \"2. Jockey/Trainer Win Rates - rolling calculations\",\n",
    "        \"3. Distance Aptitude - past performance by distance category\",\n",
    "        \"4. Surface Aptitude - turf vs dirt performance\",\n",
    "        \"5. Track Aptitude - performance at specific racecourses\",\n",
    "        \"6. Recent Form - last N race results with time decay\",\n",
    "        \"7. Class Performance - results by race grade\",\n",
    "        \"8. Weight Change - horse weight difference from last race\",\n",
    "        \"9. Post Position Stats - position-specific win rates\",\n",
    "        \"10. Pace Analysis - early/mid/late position changes\",\n",
    "    ]\n",
    "    \n",
    "    for f in features:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Processed Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save column mapping for feature engineering\n",
    "if 'df' in dir():\n",
    "    column_info = {\n",
    "        'all_columns': list(df.columns),\n",
    "        'dtypes': df.dtypes.to_dict(),\n",
    "        'shape': df.shape,\n",
    "        'missing': df.isnull().sum().to_dict()\n",
    "    }\n",
    "    \n",
    "    # Save as JSON for reference\n",
    "    import json\n",
    "    output_path = Path('../data/processed/column_info.json')\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'columns': column_info['all_columns'],\n",
    "            'shape': list(column_info['shape']),\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Column info saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `src/preprocessing/feature_engineering.py` to create engineered features\n",
    "2. Move to Phase 2: Model Building\n",
    "3. Start with position probability prediction for each horse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
